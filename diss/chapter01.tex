\chapter{Introduction}

Addressing the problem of Artificial Intelligence is very interesting but
extremely difficult, as intelligence must not only be developed, but also tested
adequately before it is possible to fully classify it appropriately.
Reinforcement Learning (RL) provides a view on intelligent behaviour that stands
on the shoulders of lots of research in fields such as neuroscience, psychology
and mathematics, and whose formalisation is mostly agreed upon in the AI
research community\cite{Sutton:1998:IRL:551283}. Reinforcement learning is used
to describe the process of learning to control the environment optimally, but
this process is not very scalable for environments that approach a level of
complexity similar to the real-world, as the agent must sufficiently solve the
problem of both creating a useful representation of sensory inputs and
generalise over it for long term learning. To tackle the representation problem
the community lately started emphasising the usage of an architecturally deeper
approach to machine learning, deep learning (DL), where efficient, scalable and
high-capacity models can learn some form of distributed representation of their
input\cite{lecun2015deep}. This subfield exploded a few years ago when some
researchers started using deep architectures such as convolutional neural
networks to approximate and represent key parts of the Rl processes, with
popular models such as modified Q-learning algorithms\cite{mnih2013playing},
trajectory/policy optimisation\cite{levine2015end} and policy representation
methods\cite{kakade2002approximately}.

\section{Games as virtual learning environments}

Through the decades several benchmark suites have been developed to allow the
reinforcement learning and AI community to build and test their algorithm in a
systematic fashion. These were usually in the form of either pure decision
problems such as the multi-armed bandit problem\cite{berry1985bandit} or
interactive and environmentally rich scenarios such as gridworld. Part of the
community has also focused on using robotic competitions to advance the state of
the art of reinforcement learning algorithms\cite{stone2001scaling}, while
others have preferred remaining within the boundaries of software and use
simulation environments to carry on their research\cite{gosavi2014simulation}.
Not unexpectedly the use of computer games as AI benchmark also has also become
common, with new comprehensive gaming suites such as the Arcade Learning
Environment (ALE)\cite{bellemare2012arcade} and open source ports of popular
RTS games such as Stratagus\cite{ponsen2005stratagus}.

\section{The Arcade Learning Environment}
The Arcade Learning Environment is a simulation tool that allows to control and
analyse several Atari games. The tool allows to extract both symbolical game
state (when available) and the screen output. Because of the simplicity of most
of the available games, the image stream usually contains all the information
necessary to win the games. Most of the available ROMs can be beaten using a
well-tuned reactive policy learnt from the input image\cite{mnih2015human}.

\section{StarCraft Brood War}
Real time strategy (RTS) games have historically been a source of complex
problems for AI researchers. The domain they represent is essentially a
simplified military simulation where players fight live and fight in a
fixed-size 2D map for the control of the resources lying all over the map to
build armies and an economy that allows them to win battles and finally the
overall game. The variety of AI and decision problems that this typology of
games involves and requires solving include (but are not limited to):

\begin{itemize}
  \item Decision making under uncertainty
  \item Opponent modelling and Learning
  \item Resource management
  \item Real-time planning
  \item Exploration/exploitation dilemma
\end{itemize}

Those are all terribly challenging problems that have spawned several techniques
and even entire fields of research\cite{buro2003real}. The creation of a machine
learning system that can at least partially solve all those problems and
therefore have the chance to be able to play with a human opponent is far from
being close, even with the relatively game-changing performances of Deep
Reinforcement Learning.

In the past years several RTS research platforms have emerged, the most
prominent ones being Stratagus, OpenRTS and Wargus, however the lack of polish,
missing game mechanics and features that are normally available in commercial
RTS games and lack of community does not make them ideal to do AI research,
especially now that some important parts of it rely also on good amounts of
available data to be fruitful. A more feasible solution was instead to take one
of the popular commercial games and adapt it as to make it an AI testing suite;
in our case, we chose the most popular RTS game every developed: StarCraft.

Starcraft is a relatively old commercial game that represents the quintessence
of Real Time Strategy (RTS) games. Its code is unfortunately completely closed
and Blizzard have never really opened the game to modifications, but luckily a
few users have developed a system to read and modify the memory of the game,
called Brood War Application Programming Interface (BWAPI)\cite{bwapi2011brood}.

\section{BWAPI}

BWAPI is a C++ API that allows to interact with StarCraft and StarCraft Brood
War. It allows to create agents by controlling directly the entire client or by
using DLL injection to dynamically run the AI code within the address space of
the game. This interface has become somewhat popular within the community and
around it have spawned a couple of tournaments and tens of developers willing to
help with its maintenance. For our purpose we had to add some functionality to
the API to allow it to be usable as a (deep) reinforcement learning environment,
such as a socket interface to be able to communicate with a Linux process and a
automatic memory saving system for collecting the visual output.