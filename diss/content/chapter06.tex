% outline discussion
% 1. Symbolic restructuring
% 2. DQN results 
% 3. Problems with StarCraft
% 4. Can we make the interface better? 

\chapter{Discussion}

\section{Project Completion}

Despite the short amount of time that we had at disposition to complete the
project, we believe we have successfully implemented all the initial specifications:

\begin{itemize}
\item We built a robust interface to StarCraft.
\item We managed to connect such interface to a state-of-the-art tensor library.
\item We implemented a couple of baseline reinforcement learning algorithms to
  work with simple StarCraft domains.
\end{itemize}

All of this was done with limited experience in network programming and no prior
knowledge of BWAPI, Windows API programming or Torch. The downside is that the
development of the entire pipeline took a much greater amount of time than
expected (and well over twice the assigned 400 hours), most of which ended up
being spent in the implementation of the various internal interfaces between the
different modules and environments.

Moreover while the initial idea was to create modules to cluster the useful
parts of the game state in such a way that researchers would only have to
interact with the client code when ``configuring'' the reward function
\footnote{If needed. In reality this can already be done in most cases on the
  agent side with the aid of the game state data.}, some of the initials design
decisions made the implementation significantly harder to modify and debug. This
became especially evident when we realised towards the end of the project that
to add the necessary degree of control over the game settings we would have had
to change the structure of BWLE's modules to allow the server to interact not
only with BWAPI but also with the actual StarCraft process.

\section{Challenges for machine learning approaches}

Now that the framework is in a usable state, it's time to think about tackling
some of the fundamental problems in reinforcement learning and general
artificial intelligence.

\subsection{State exploration as a policy}
In Chapter 2 we mentioned that most of the roms used within ALE provide problems
in which the visual data contains the full state of the game (or most of it).
That is, assuming we have a good understanding of the score function, the image
given by ALE essentially gives us all the information we need to model the most
useful action at any given time with reasonable results. In StarCraft the visual
feedback purely represents the state of the game at a specific point in the map
- point usually chosen by the player through keyboard or mouse actions - and
unless the user puts time and effort in dynamically exploring the map to update
his world knowledge, they can't get an idea of the general state of the game.
The players are given an extremely compressed representation of the state of the
game through minimap on the bottom left, but that representation only gives a
general idea of the trend of the game (Figure \ref{fig:one_star}).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{ch6/star_one}
    \caption{Standard StarCraft game screen that the player needs to interact
      with. The minimap is on the bottom-left; black areas are not yet
      discovered (and therefore not observable), grey areas have been discovered
      but are not fully observable anymore (topology of the map is known, unit
      or resources status is not) and light brown areas are observed areas with
      full observability in terms of game state (symbolical). The white
      rectangle signals the visible area at the current window position.}
    \label{fig:one_star}
\end{figure}

Additionally the minimap is also affected by the so-called \emph{fog of war}, a
feature in most RTS games that prevents players from observing the state of
locally explored but unobserved zones at a specific time. This means that to
gain the victory the player is forced to continuously explore the state of the
game, regardless of whether they already have a more or less complete notion of
the topology of the map. In reality, Starcraft allows us to cheat and disable
the fog, making exploration a one-shot process i.e. once the map is explored
it’ll be “observable” through the entirety of the match.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ch6/double_star}
    \caption{The variance in visual input between the two windows is as seen
      very high, because not only the relative position of the unit changes, but
      the entirety of the space changes significantly.}
    \label{fig:double_star}
\end{figure}

We can say that StarCraft’s visual feedback by itself (i.e. even without encoded
features) is therefore essentially non-Markovian. Starcraft players are required
to move the window to gain information, and the image changes drastically
depending on where the window is (Figure \ref{fig:double_star}). Unless the
agent is provided with some form of memory to collect the game state, we cannot
only use the visual data as input. Our hypothesis is that creating an encoded
representation of the entire state of game is by itself an unsolved mixed deep
learning (representation) and RL problem, as the variance in the input is simply
too large and it's entirely based on the learnt policy for both unit actions and
screen/environment/player actions.

\subsection{Hierarchical Decision-Making}

In Chapter 2 we have discussed some work in Hierarchical Reinforcement Learning
that could be used in principle to tackle StarCraft, however no particular model
seems to fit all the requirements of the platform, especially if we take and
end-to-end approach to the problem. A possible approach would be to construct a
partial program using the developer knowledge to guide the design and using
reinforcement learning to learn the parameters of the sub-plans
\citep{marthi2005concurrent}, working towards slowly replacing all the manually
engineered parts of the plan by doing options modelling.

Very recently DeepMind released AlphaGo, a system that mixed Monte-Carlo Tree
Search with Deep Reinforcement Learning \citep{silver2016mastering} to
successfully beat the current World Champion of Go. It's probably fair to
speculate that a reinforcement learning framework will need some amount of
strong supervision with respect to exploring the space and hand-tuned feature
extractors to learn good overall tactics.

% Cover Russell work
% Cover Policy reuse
% include also policy distillation?
% Cover counterfactual reasoning

\subsection{Data-Efficient Machine Learning}

Reinforcement learning and deep learning algorithms are alike in the
fact that they require many data samples to correctly estimate underlining
models, in fact those systems have now become hugely popular as scalable
solutions for tackling problems related to object recognition, image
segmentation and speech processing, all of which have relevant data available in
great quantities.

If we look at most StarCraft games we can notice instead that while some
strategies are predominant in certain parts of match (e.g. build workers and
scouts to explore the map and gather resources during the early stages of the
game), a lot of strong tactical moves are extremely rare and will appear only
under precise conditions. This is why we must develop interactive mechanisms to
use those powerful techniques in a one-shot fashion. The machine learning and
robotics communities has started tackling this problem
\citep{deisenroth2015gaussian, assael2015data, kingma2014semi,
  rasmus2014denoising}, but no clear solution has emerged so far. StarCraft
together with our framework should provide the perfect environment to explore
those scenarios.

\section{Future Work}

Before releasing the platform there are a few things we need to carry out to
make the user experience more straightforward.

\subsection{Design Clean-up}

Now that we know the quirks of both BWAPI and StarCraft itself, we can adjust
the design of the client manager modules to better match BWAPI's internal
structure. Here's some ways we could achieve this:

\begin{itemize}
\item The client could be split in three different asynchronous modules: a
  StarCraft handler to control Windows and the StarCraft process, a Game State
  collector to observe and collect the state using BWAPI and WinAPI, and a
  networking interface to deal with the external interface.
\item \texttt{BWAPIClient} could be expanded to add the ability to fully control
  the game in a similar fashion, simplifying our overall architecture. This way
  we could easily write a background daemon process to produce BWLE virtual
  machines with underlying BWLE processes running all the time, facilitating
  multi-client experiments.
\end{itemize}

Additionally we would like to replace the protobuf serialisation system with
some slightly more used serialisation mechanisms such as \texttt{Json} or
\texttt{XML}. This would be helpful to integrate the architecture with web
systems and would also allow a more easily accessible (and human readable) data
representation by potentially only losing some speed when parsing the structure.

\subsection{Full Linux compatibility}

The great majority of AI and machine learning researchers do not use Windows as
their development environment. Having to run a VM to host the StarCraft client
and the interface means that an unusually high usability cost is associated with
using the platform (that is, providing a simple installation script or a working
out-the-box environment is not an immediate process).

In the past some parts of BWAPI were made to be compatible with other compilers
and WINE, a Linux environment that tries to reproduce the Windows API and
provide tools for running Windows software on unix platforms. Porting the
entirety of our pipeline over Linux would mean being able to significantly
simplify the integration with other state-of-the-art machine learning systems
and remove some of the difficulties in using the platform (as the current
installation can be cumbersome at times). This is fundamentally our next
immediate development step.

\subsection{ROS integration}

ROS is an open-source framework designed to aid the development of robotic
software. It is essentially an extremely useful collection of build tools,
programming languages interfaces, simulation and visualisation software and
packages that allow the robotics community worldwide to share research with a
unified interface.

A wide variety of robotics communities regularly release packages for ROS, and
while most of them are not directly applicable to the StarCraft domains, it
would be extremely beneficial to link the platform to research groups work on
hierarchical planning and reasoning. As ROS is available on both Linux, Windows
and OS X, it would also allow us to define a unified and distributed networking
interface based on the publisher-subscriber messaging pattern and greatly
simplify the creation of a StarCraft-based simulation environment.


\section{Chapter Summary}

Over the course of this project we have learnt that StarCraft is a challenging
domain for a large variety of reasons, most of which consisting of problems
effectively shared with many other RTS games. The platform has been equipped
with a solid software framework, and can now be used to study reinforcement
learning, general machine learning, and mult-agent systems. We would not have
managed to reach the same results using any of the other available RTS platforms
had we not got something equivalent to BWAPI, so we believe we made the right
choice at the start of the project when we decided to focus on StarCraft.
 