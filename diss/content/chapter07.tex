% outline conclusion
% 1. Conclusion of scoping work

\chapter{Conclusion}

Finally reaching the end of this project, we can say that we have reached an
extremely satisfactory point in the development of a new agent learning
platform. Not only we have met and gone past our initial engineering goals, but
we have also managed to show that it is possible to learn interesting policies
using both standard Q-learning and Deep Q-learning when StarCraft is reduced
into a simple MDP setting, and we have lain down the algorithmic baseline for
our next experiments in the area. We now believe that it is essential we start
designing policies with hierarchical structures in mind from the start.

Designing, implementing and testing the platform took well over 800 hours, while
a couple of other hundreds of hours were spent reproducing different
implementations of Q-learning and Deep Q-networks. Unfortunately a lot of time
was spent refactoring and dealing with the complex - and sometimes buggy -
behaviour of Windows APIs and StarCraft, however our efforts paid off by giving
us an extensible and useful platform.
