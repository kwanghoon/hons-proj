\chapter{Results}

\section{Stress Tests}

When we started developing BWLE we were already aware that many experiments were
running on ALE for entire days without stopping \citep{mnih2015human}. Because
of the huge amount of steps steps required to learning policies based on some
deep architecture, we decided it was worth run a few stress-tests as well. 

The experiment consisted in having a basic agent send random actions to many
units for 5 million steps. We used the synchronous communication system and
fixed the observation-action rate per minute to 600 (10 Hz), ordering the client
to restart the game every 10000 iterations. The map consisted in an empty plane
with a few unkillable controllable units and the same amount of unkillable
aggressive enemy unit (as to force the game to do at least some planning all the
time).

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ch5/cpu_test_re}
    \caption{CPU usage of BWLE with respect to the Windows client and the Linux
      server. All the usage and was synchronously recorded every 10 steps. The
      memory usage is not shown because it completely overlaps with the CPU
      usage.}
    \label{fig:fst_usage}
\end{figure}

In terms of efficiency, Figure \ref{fig:tree} shows that the client spends the
majority of the time in idle waiting for action messages from the agent server.
For an hour we allowed the interface to run as fast as possible, reaching 18
``iterations per seconds'' when sending the image and 24 when sending only the
game state, both of which are reasonable targets (as they are way faster action
rates than ones used by the average professional StarCraft player).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{ch5/client_split}
    \caption{Output of the Visual Studio 2013 C++ Profiler showing the most used
      calls in the Windows client. Ignoring the calls made to the stream
      operators (as we already provide a way to disable all printing from BWLE's
      configuration), we can see that the client spends most of the time waiting
      for responses from the server.} 
    \label{fig:tree}
\end{figure}


The recorded usage (Figure \ref{fig:fst_usage}) shows a completely stable system
that can deal with both simple and complex maps using relatively few
computational resources. We did not achieve 100\% of branch coverage because we
didn't allow the usage of all the available actions, but the majority of the
available client interface got called at least once (according to Visual Studio
2013 92\% of branch coverage was reached).

It must also be noted that most StarCraft AI tournaments
\citep{ontanon2013survey} run BWAPI-based clients for entire weeks without any
noticeable problem, so the results we obtained were not completely unexpected.

\section{Navigation Task}

Next we tested our algorithms on the first StarCraft task, the``Navigation
Task''. The obtained results mostly matched our expectations: the task consisted
essentially in a medium-size grid-world which both algorithms managed to
eventually converge. Figure \ref{fig:nav_task_results} shows the average reward
per episode, where an episode is defined as the sequence of steps from the
starting state to a terminal state. The policies were evaluated every 10000
iterations by playing 10 episodes, recording mean and standard deviation.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ch5/nav_task_re}
    \caption{Results of DQN and Q-learning on the simpler navigation task.}
    \label{fig:nav_task_results}
\end{figure}

Q-learning was tested with three different learning rates, while DQN was tested
with the same parameter configuration employed in \cite{mnih2015human}. We tried
to do a more appropriate sweep in the parameters space, but DQN tended to
diverge when varying too much some of the parameters, and it would have taken
too much time to proceed with additional testing. Changing the action
persistence window size significantly impacted the learning process, as small
windows made movement actions too quick for the agent to switch from one block
of the grid to the other, rendering the task essentially partially observable.
We know that while DQN is stable for MDPs, states with imperfect information in
general require some stochastic strategies to reach policies that are close to
being optimal \citep{heinrich2016deep}.

\section{Guided Navigation Task}

The experimental setting was similar to the previous experiment, beside having a
different map and reward function. DQN was trained using all the 20 available
maps, while Q-learning had to learn and perform exclusively on one map. This
choice was made to avoid having to find the correct parameters values, as the
map was much larger and we couldn't feasibly decrease the resolution of the grid
without changing the map's walkable paths average width.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ch5/guid_task_re}
    \caption{Results of DQN and Q-learning on the guided navigation task.}
    \label{fig:guid_task_results}
\end{figure}

Figure \ref{fig:guid_task_results} shows the learning curve similarly to the
previous experiments. We can see that after around 400000 iterations the total
reward starts to converge, and that overall DQN seem to be reaching the optimal
policy at some point (differently to Q-learning, which seems a bit slower and
possibly converging to a sub-optimal behaviour).

\section{Survival Task}

Our last experimental setting was also the hardest. We hypothesised that both
algorithms would learn to survive by fighting, but we also expected the CNN to
acquire policies invariant to the units positions and more correlated to the
amount of units. We also expected to see more variance at evaluation time due to
the uneven difficulty of the designed maps.

% TODO Write the need to remove outliers.

Figure \ref{fig:surv_task_results} shows the results obtained by running
multi-unit Q-learning and multi-unit DQN. As predicted, the standard deviation
of the episodic reward arrays ended up being much higher compared to the other
tasks because some of the maps were significantly harder than others (as some
units were doomed to die). We can see that the average amount of steps in an
episode gradually increases, which means that learning is happening.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ch5/surv_task_re}
    \caption{Results of DQN and Q-learning on the harder Survival task.}
    \label{fig:surv_task_results}
\end{figure}

We inspected the collected action data and we noticed that the distribution of
actions taken by each unit differed significantly depending on the amount of
allied and enemy units. This was probably due to the fact that the Convolutional
Neural Network built filters invariant to the position of the units, making it
more likely to try and shoot other units when close to allied units. Finally,
at the end of the training session, the units lasted on average 50 iterations
more than at before training.

\section{Chapter Summary}

In this chapter we presented the results of the experiments described in Chapter
4. We confirmed some of our initial hypotheses and tested the platform to its
limits, providing a baseline for future design and research work. Ideally we
wanted to also implement a couple of model-based reinforcement learning
algorithms, because they are usually more suited to partially-observable domains
and they can potentially learn the hierarchical structure of the StarCraft as
they play the game. Unfortunately we didn't find the time to properly review the
topic and choose some algorithms within our time constraints.
