\chapter{Platform Design and Implementation}

As mentioned in Chapter 1, the goal of this thesis was to develop an interface
for reinforcement learning research on StarCraft. The following sections
outline the process of design, implementation and testing of the platform.

\section{Specifications}

Obtaining a list of specifications for the platform was relatively challenging
for a few reasons: the design needed to provide a powerful environment while
maintaining a strong degree of expandability. With the sudden re-popularisation
of reinforcement learning the community has begun challenging previously
untouched problems, meaning that entirely new architectures might soon appear
and provide new constraints for existing platforms.

We identified the following minimal requirements:

\begin{itemize}
\item Ability to control StarCraft. This included (but was not limited to)
  starting, pausing, and ending games, killing and re-starting the program,
  selecting certain maps or campaigns.
\item Ability to collect and share game state information.
\item Ability to control the game from the player perspective
\item Ability to collect data from human players and replays available over the
  internet.
\item Ability to use hacks and / or some of the cheats the deactivate some of the
  features that make the general game particularly hard (say, fog of war). 
\end{itemize}

Additionally, because of the surge of deep reinforcement learning we placed the
ability to use one or some of the currently popular deep learning libraries on
top of the requirements list.

% Overall design

\section{Brood War Application Program Interface}

One of the main reasons this project became possible within the time constraint
was because of the existence of the Brood War Application Program Interface
(BWAPI). A closed-source game like StarCraft would normally be very hard to
control and to use as a learning platform, but thanks to a few extraordinary
developers it has been possible to build bots for game. BWAPI is a framework
written in C++ that provides a clean and modular interface to gain access to the
game data structures, allowing to collect game information and to control units
much in the same way a human player would.

BWAPI uses DLL injection to interact with the game, providing two interfaces to
the game: BWAPIModule and BWAPIClient. The first one allows to control of the
units through callbacks fired 
% (?????) 
by in-game events, so it's mostly suitable
for running agents that do not require to have complete control of the game (as
the game is configured centrally by the injected BWAPI.dll at startup). The
second interface allows to directly connect and interact with the server started
within BWAPI.dll, which gives the ability to arbitrarily control most of the
process state.
Even thought BWAPIClient lacked some documentation, we decided to use it to give
us the maximum amount of maneuverability later in the implementation phase.

\section{Pipeline Design} % 2 pages ()

StarCraft and BWAPI by default are only supported on Windows platforms, but most
of the critical machine learning and tensor libraries are actively developed
only on Unix systems. While the situation has gradually seen some improvements
with the introduction of ... and ..., most of the current state-of-the-art
development and research happens on Torch, Theano and Caffe, most of which are
widely supported only on Linux and OS X. Given these constraints we decided to
separate our development in two macro-systems, one running on Windows and
directly communicating with StarCraft and the other running on Linux and
interfacing with Torch.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ch3/arch_overall}
    \caption{Overall architecture. The interface running on Windows 7
      communicates with StarCraft using BWAPI, and connects to a Linux server
      using standard non-blocking sockets. The Linux interfaces communicates
      with an agent through shared objects, }
    \label{fig:arch_ov}
\end{figure}

The two separate systems communicate between each other using non-blocking
sockets in a synchronous or asynchronous fashion depending on how the interface
is configured. The choice is dependent mostly on the experimental design:

\begin{itemize}
\item synchronous communication to simulate the standard MDP-like setting where
  at each step all agents receive the entirety of all observable data, the
  environment is blocked until the decision process has finished and the
  environment executes a step when the MDP processes send a certain command. 
\item asynchronous communication to have a closer simulation to real-world
  scenarios and games. In this mode the game executes each step at a fixed rate
  and sends information as fast as possible (as the game steps rate can be much
  faster than it's currently possible to process and send the visual data). The
  agent can then send actions at any point of the process, which are then
  executed in the next immediate step.
\end{itemize}

To be as efficient and fast as possible, the data structures containing the game
data are serialised using the Google protobuf library \citep{varda2008protocol}.
The library provides a way to generate serialised objects using pre-defined
schemas, which can then be filled with game information and sent over as
compressed strings. Unfortunately the library is not optimised for very large
objects, so we in addition implemented our own serialisation method for
sending the image structure over the socket connection.

While both systems works, the asynchronous interface requires a significant
amount of work on the agent side to sync the data sources and take into account
the perception/action delay. Such processes are generally considered a source of
research problems, but they are not of great interest to decision making, as
once the system is modelled it just becomes part of the environment.

For the sake of clarity and brevity we will from now on describe the rest of the
system only with respect to the synchronous communication, as the implementation
of the rest of the modules is mostly independent from this choice and it greatly
simplifies the agent interface.

\subsection{Windows Interface}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ch3/arch_win}
    \caption{Overview of the architecture on the Windows sub-system.}
    \label{fig:arch_win}
\end{figure}

Implemented in C++

Automatically starts the game and injects BWAPI.ddl using ddlinjector. 

\subsubsection{StarCraft Controller}

BWAPI configuration is parsed and edited according to variables specified in the
client configuration system. The idea is to completely hide BWAPI and present a
unique and coherent configuration system with useful defaults.

A list of maps can be load in the system by specifying a file to be parsed. The
game can then be set to load maps sequentially or in random order, which is an
essential feature for standard training procedures.

Games can be interrupted, terminated or restarted at any time. The client allows
to either completely kill StarCraft, therefore resetting its state with a
process-wide restart, or to just use the in-game restart function to quit the
game and start a new one. The first method allows to change certain settings
that can be loaded only when StarCraft is started, but it's significantly
slower, as it takes around two seconds on a quad-core machine from issuing the
kill command and the first step of a new game. The second method takes around a
second to start another game. % CITATION NEEDED (check starcraft times)

% TODO: add seed to game if possible, otherwise future work

% insert example files here 

\subsubsection{Screen capturing}

% TODO describe window size

BWAPI does not provide any interface to capture the visual output of the
game\footnote{Some work had been previously done to integrate some form of
  streaming functionality within BWAPI, but it was dropped in favour of existing
  tools. See https://github.com/bwapi/bwapi/issues/596}. To fulfill the
specifications we developed a module to capture the StarCraft window and include
it in the game state.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ch3/capt_conv}
    \caption{Conversion process of the visual data. The raw pixel data captured
      using the window handler gets stripped of superfluous data and separated
      by channel at the same time.}
    \label{fig:capt_conv}
\end{figure}

To achieve this functionality we used the Graphics Device Interface library
Gdiplus, included as part of the standard Windows API, to find the StarCraft
process, grab its window's handler and extract the raw pixel data. The obtained
data structure stores the pixel in an array of 32bit integers, where integer
represent the row-wise RGB data and a null byte. 

Before serialising the data to use the network endianness order over the wire,
we strip the null byte and we cluster the channels separately as to obtain first
all the red channel values, then the gree channel values, and finally the blue
channel values (Figure \ref{fig:capt_conv}). It must be noted that while this
process has been made as fast as possible on the single thread (given the frame
size), it could be even further optimised by using the fact that the channels
are essentially already independent and therefore using a standard parallel loop
and some pointer map to shift the array around.

% TODO describe image serialisation

\subsubsection{Collecting the Game State}

Collecting the game state represented a non-trivial design and implementation
challenge. BWAPI is designed with in mind the process of writing classical AI
bots, which usually does not include getting access to the entirety of the game
data structures. This means that the interface is modular and divided with
respect to unit classes, weapons, available upgrades, events, forces and so on.
Collecting the game state in a straightforward manner means therefore querying
most of the internal interface, filtering out errors and unneeded information,
and finally clustering everything to fit our shared data structures. 

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Write here the result }
 initialization\;
 \While{While condition}{
  instructions\;
  \eIf{condition}{
   instructions1\;
   instructions2\;
   }{
   instructions3\;
  }
 }
 \caption{How to write algorithms}
 \label{alg:state}
\end{algorithm}

To do so we developed a module called \texttt{StateManager} to take the burden
of doing all the necessary operations. Algorithm \ref{alg:state} gives an
overview of the process. Once the state is collected, the module serialises it
and sends it over with the previously collected image. As described in the
section introduction, we use a protobuffer so that it can be sent over the wire
and parsed by the server running on the Linux end. \texttt{StateManager} is
written in such a way that adding units or information to the unit data
structure can be easily achieved by adding entries to the proto file (which
specifies the schema to use to build the C++ interface) and modifying
\texttt{StateManager::setGameState\_} to include the new information into the
protobuffer.

The design was chosen to be purposely centered around the unit data structure
because most of the information available to players is effectively coupled to
the visible units. Everything else can be either inferred or learnt by playing
the game. We have for instance purposely excluded available information about
players, races, and upgrades because they didn't provide any particularly useful
information, however it wouldn't take a huge amount of additional work to
identify other useful types of data that should be included by default in the
platform.


\subsubsection{Controlling units}

To centralise the control of units we designed and implemented another manager
module, called \texttt{AgentManager}. Every time a ServerPacket received, the
module extracts the action identifier, the unit associated to the action and any
parameters coupled to it. To allow for unit-less actions, we allowed the
controlled player to correspond to a negative id (usually -1).
% TODO add parameters to action

One of the main issues of building agents for StarCraft (and most of RTS games)
is that the available actions are entirely context dependent. 
For instance:

\begin{itemize}
\item the generic action \texttt{attack target} becomes \texttt{heal target}
  when the executing unit is a Terran Medic.
\item without workers, a \texttt{mine target} action is completely invalid and
  cannot be executed. That is also the case when workers are present but the
  target is a unit or a building (in which case the action becomes most of the
  times ``attack'').
\item buildings (and some units) have different upgrades or sometimes no
  available upgrades at all, so a \texttt{upgrade to id} action can have a wide
  variety of effects.
\end{itemize} 

We couldn't find a way to quickly tackle the problem within the project
time frame, so we created a few different types of actions based on some
particular scenarios, making sure to designing them to be as general and
reusable as possible. We ended up with three classes of actions:

\begin{description}
\item [Player Actions] - Those actions mostly include game-wide primitives such
  as \texttt{move screen [up|down|left|right]}, and mouse controls such as
  \texttt{click [left|right][pos]} and \texttt{select [area]}.
\item [Basic Unit Actions] - TODO
\item [Complex Unit Actions] - TODO
\end{description}

The \texttt{Player} module then takes the previously collected \texttt{UnitSet}s
and orders the matching units some particular action.

Our implementation also allows to reuse the specified primitive actions by sending
multiple action ids within the packet. 
% implemented also into the agents code
% on linux!
Care was taken to make sure that the system was robust enough to ignore killed
or blocked units during the action steps.

As a consequence any created unit during this timeframed is ignored.

\subsubsection{Generating Rewards}

Designing reward functions in RTS games like StarCraft is another problem that
has many possible but inter-incomparable solutions, making it problematic to
find a generic reward function that covers all possible scenarios. In
particular, when we inspect games played by human players we can imperically
observe that the ideal reward function might also need to be somehow engineered
to be hierarchically structured, as the ``goodness'' of a state-action pair is
often entirely context dependent and changes depending on whether it's being
estimated over a local micro-scenario or the entire game. Let's look at the
structure of a common competitive game between two players.

It is a widely recognised fact that most board and computer games can be divided
in three major phases, each usually possessing an almost completely isolated
meta-game from the others. Those phases are often called early-game, mid-game
and end-game. In StarCraft, the early game is by design forcefully dominated by
a combination of exploration, resource-gathering and build-order optimisation.
The mid-game is the most variable of the three, and includes making strategic
and tactical decisions to optimise (and protect) a chosen build-order with
respect to a roughly constant resource income income rate. This phase of the
game significantly changes depending on the strategy of both players and the map
topology. Finally, the end-game is the last phase of the game and mostly
consists in the winning player successfully carrying their final part of the
strategy, or the losing player making a comeback thanks to either errors of the
opponent or moves that are carefully-thought but often containing some amount of
``gambling''.

In reality it can also be observed that professional StarCraft players behave
similarly to chess and go players, where the meta-game tends to rely on
particular pre-determined dynamic build-order strategies to choose which tactics
to use when variations of known scenarios appear. At high levels very often the
sequence of actions becomes so close to be optimal (considering human
limitations) that the game effectively is won by which strategy is strong
against the other (in a rock-paper-scissors fashion).

All of those factors contribute to complicate the design of a general purpose
reward system, especially if the final goal is to be able to play good games
against both amateur and professional players. Some research has explored
learning the reward function using techniques like Inverse Reinforcement
Learning\citep{ng2000algorithms}, but none of the explored domains are even
remotely comparable to StarCraft's extremely large state space.

% TODO Put example image here

To get around the problem we designed a simple module called
\texttt{RewardManager}, whose goal mostly consists in analysing the current
state (or a history of the states) and providing a numerical reward. Depending
on the configuration the module can also output a array of values for each unit,
providing support for multi-agent settings or some hierarchical reinforcement
learning methods such as Q-decomposition \citep{russell2003q}. Once the reward
is calculated it's then taken by the \texttt{StateManager} to be inserted into
the protobuffer and sent over.

% TODO add that more work is needed

\subsection{Linux Interface}

The linux system was also implemented in C++. 

We mostly focused on designing a reusable and generic interface, but we tried to
offload most of the decisions involving the domain to the protobuf data
structure and the StarCraft client interface, so that it
would later be easier to link our platform to different language environments or
even other games. 

One of the main goals we had in mind for this part of the project was in fact to
reach a point in which adapting the interface for the language would become just
a matter of defining some symbols and loading a shared object library. This is
why why decided to implement the shared interface also using C++, since most
mainstream programming languages can load C/C++ symbols either through specific
systems such as Foreign Functions Interfaces (FFIs) or through standard
libraries.

Several different tensor libraries were reviewed to pick one for this project's
time frame:

\begin{description}
\item [Caffe] - Machine learning library developed by the Berkeley Vision and
  Learning Center. Initially designed purely for computer vision research
  applications, its support for other input types is not particularly strong (or
  is often just absent). It makes heavy use of protobuffers to generate neural
  network architectures and it supports both a Python and a C++ interface. One
  of the most cited reasons for choosing this library over the rest is because
  of the popular Model Zoo: a continuously updated collection of trained models
  from recent NIPS, ICCV and CVPR papers that can be all used for comparison
  purposes.
  
\item [Theano] - One of the oldest machine learning libraries focused on deep
  learning. Written in Python using numpy to support fast mathematical
  operations, it has spawned several high-level deep learning frameworks and is
  currently one of the most popular tensor libraries. The initial design was
  produce with in mind having to support the variety of use cases it later got
  used for, so the library has grown somewhat unevenly, making it somewhat
  bug-prone and difficult to contribute in comparison to other open source
  libraries.

\item [Torch] - Tensor library written in C and Lua mostly used by Facebook,
  Google DeepMind and a few other companies \cite{soumith}. It relies on LuaJIT,
  a Just-In-Time interpreter that allows Lua to run at speed comparable to pure
  C code. It has a modular framework that makes it easy to develop non-standard
  layers and architectures. While Lua is a relatively small and readable
  language, its community is small and doesn't compare to the Python, C++ or
  Java communities; this makes the language less attractive as a whole for
  research because of the lack of general-purpose libraries.
\end{description}

We chose Torch because at the time (July 2015) the majority of
available Deep Reinforcement Learning code had been written on top of it. Since
then the available frameworks focused on tensor-based computation and deep
learning nearly doubled (see for instance Microsoft's CNTK, Google's TensorFlow
and Berkeley's CGT), so the choice would not have been as straightforward had it
been more recent. That said, we still believe our choice to the best one for the
project, considering it effectively forced the C++ interface to be modular right
from the start.
 
\subsubsection{C++ Interface}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{ch3/arch_linux}
    \caption{Overview of the architecture of the Linux sub-system.}
    \label{fig:arch_linux}
\end{figure}

We designed the Linux C++ interface to be complementary to the Windows client.
The interface started simply as a standard socket server and subsequently
evolved into a fully-fledged controller after adding a protobuffer parser, a
controller class and a storage system. Like the client, the server system
supports both synchronous and asynchronous modes, using non-blocking sockets and
a couple of threads.

The overall system is coupled with the client and requires to updated the
generated protobuffer interface every time the proto configuration file is
changed in the client. Because of this we couldn't escape from adding the
library as a dependency, but this update was made painless by scripting the
whole process.

The system is split into three different modules:
\begin{description}
\item [Server] - Manages the connection with the client. It's the first object
  that the interfaces initialises, so it blocks the execution of the process
  until at least a client connects to it. The module was designed from the start
  to support multiple sockets and clients, but we didn't find the time to spin
  more than 2 instances of StarCraft on the cloud and run an heavy stress-test
  for this functionality.
\item [RingBuffer] - To provide a native way of storing and managing long-term
  information received from the client, we implemented a module to act as a
  middle-ware storage system based on a cyclic buffer. The history length can be
  changed when initialised and it's optimised so that the operation of switching
  data around the buffer is as fast as possible (this was done by manipulating
  pointers).
\item [BotInterface] - Essentially acts as the manager class for the whole interface.
  It starts the server, it initialises the data structures for the ring buffer
  and the temporary protobuf parser, and it provides methods to interact with
  the client.
\end{description}

Once we developed and tested the C++ interface, we had to create a LuaJIT
wrapper around it to allow our lua agents to use it.

\subsubsection{Lua-compatible Interface}

LuaJIT provides an FFI library to wrap C/C++ objects and their interfaces. The
library provides a way to parse C declarations, which can then be used to read a
compiled shared object file containing the C objects definitions. This allows to
integrate C and C++ code (exported as C) very quickly, without any need to
manually write language bindings in Lua. The Just-In-Time compiler generates
then instructions that are roughly as efficient as the ones obtained by standard
C compilers like GNU \texttt{gcc} and \texttt{Clang}.

Arguably the most interesting part about our interface is that it provides a
method to take the raw pixel data sent by client and transform it into a Torch
tensor that can then be used in our lua agents. This was possible by using the
Torch C API provided with the framework, which allowed us to copy the obtained
from the C heap to the LuaJIT storage system. We therefore became able to treat
our image as a Torch object and take advantage of the provided Torch
\texttt{image} library.

\subsubsection{Connecting the agents}

To provide the interface to the agent we used the FFI library to wrap the C++
objects, wrapping the resulting functions into a class using Torch's
\texttt{Class} library, finally creating the \texttt{bw\_interface} module.

We didn't wrap the entirety of the C++ functionality because we decided to
implement the ring buffer functionality in our agents as well, as to match the
common interface available to agents running on ALE for the sake of consistency.
The final accessible version of the interface to the agents ended up therefore
containing the following functions:

\begin{description}
\item [update\_data(void)] -
\item [send\_packet(void)] -
\item [gen\_packet(bool, bool)] -
\item [add\_action(int, int*)] - Takes a unit id and an array of action ids and
  adds it to the list of actions.
\item [update\_data(void)] -
\item [get\_image\_tensor(void)] - Returns the last stored 
\item [get\_packet\_string(void)] -
\item [get\_last\_turn\_data(void)] -
\item [get\_state(void)] -
\item [step(table, bool)] -
\item [new\_game(bool)] -
\end{description}
 
\section{Summary}

In this chapter we have outlined the motivations behind different chosen
designs, we have described our implementation, and we have covered the algorithms
used for testing and baseline construction.

Designing, implementing and testing the platform took well over 800 hours, while
a couple of other hundreds of hours were spent reproducing different
implementations of Q-learning and Deep Q-networks. Unfortunately a lot of time
was spent refactoring and dealing with the complex - and sometimes buggy -
behaviour of Windows APIs and StarCraft, however our efforts paid off by giving
us an extensible and useful platform.